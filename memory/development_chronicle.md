# Bounding Box Adjustment Model: Development Chronicle

## 1. プロジェクト概要

本プロジェクトの目的は、論文「Snapper: Accelerating Bounding Box Annotation in Object Detection Tasks with Find-and-Snap Tooling」で提案されている「Bounding Box Adjustment Model」を再現実装することです。このモデルは、ユーザーが提供するラフなバウンディングボックスを、対象物体に精密にフィットするように自動調整する機能を提供します。

*   **入力**: 対象物体が写った画像1枚と、その物体1つに対するラフなバウンディングボックス座標 (x_min, y_min, x_max, y_max)。
*   **出力**: 入力されたラフなバウンディングボックス座標を、対象物体にぴったりと調整したバウンディングボックス座標。

## 2. 初期アイデアと検討段階 (推定)

開発初期においては、論文のFigure 3や関連情報を元に、モデルの構造やアプローチについて様々な検討が行われたと推測されます。

### 2.1. 基本的な方針の模索

*   **ResNetベースの特徴抽出**: 当初から、強力な画像特徴抽出器としてResNet（特に事前学習済みモデル）の利用が念頭にあったと考えられます。
*   **座標回帰 vs. 位置確率**: バウンディングボックスの4つの辺の座標を直接回帰するアプローチと、各ピクセル位置が辺である確率を予測するアプローチが比較検討された可能性があります。後者のアプローチが、より精密な調整に適していると判断されたと推測されます。

### 2.2. モデルアーキテクチャの初期案: ResNet最終層の利用

初期の検討段階、あるいは論文解釈の初期段階では、よりシンプルなモデル構成として、ResNetの最終特徴マップのみを利用する案があったかもしれません。

*   **アーキテクチャ案**:
    1.  入力画像（ラフなBBoxでクロップしたもの）をResNetに入力。
    2.  ResNetの最終層から得られる特徴マップ（例えば、2048チャネルの7x7など）を取得。
    3.  この特徴マップをフラット化、あるいはGlobal Average Poolingを適用。
    4.  全結合層（FC層）を通して、4つの座標値 (Δx_min, Δy_min, Δx_max, Δy_max) のオフセットを出力。あるいは、直接座標値 (x_min, y_min, x_max, y_max) を出力。
*   **課題**:
    *   空間的な詳細情報が失われやすく、精密な辺の位置特定には不向きである可能性。
    *   ResNetの最終層はクラス分類タスク向けに最適化されており、位置特定タスクには必ずしも最適ではない。

このアプローチは、実装の容易さから初期のプロトタイプとして考えられたか、あるいは論文の解釈が深まる前の段階での仮説であった可能性があります。しかし、Snapper論文の趣旨である「精密な調整」を実現するためには、より詳細な空間情報を扱えるアーキテクチャが必要と判断されたと考えられます。

## 3. モデルアーキテクチャの進化と現在の実装

より精密なバウンディングボックス調整を実現するため、論文の詳細な記述や関連情報（特にAWSのブログ記事）を参考に、現在のモデルアーキテクチャが形成されました。核心的なアイデアは、「辺の位置予測をピクセルごとの分類問題として扱う」という点です。

### 3.1. Snapper Bounding Box Adjustment Model (現行)

現行モデルは以下のコンポーネントで構成されます。

1.  **特徴抽出 (Feature Extraction)**:
    *   ResNet-50（ImageNetで事前学習済み、最終的な全結合層と平均プーリング層は除く）をバックボーンとして使用。
    *   論文で提案されているMulti-Scale Featureの利用を試みるため、FPN (Feature Pyramid Network) のような複雑な構造ではなく、まずResNetの異なる深さの層から特徴マップを抽出・組み合わせるアプローチを採用。
    *   具体的には、`resnet.layer3` (出力チャネル数: 1024, 解像度: 入力画像のH/16, W/16) と `resnet.layer4` (出力チャネル数: 2048, 解像度: 入力画像のH/32, W/32) の両方から特徴マップを取得します。
    *   これらの特徴マップは、後続の処理でそれぞれ独立して扱われた後、最終的に統合されます。

2.  **方向性空間プーリング (Directional Spatial Pooling)**:
    *   `layer3` から得られた特徴マップと `layer4` から得られた特徴マップそれぞれに対して、独立に方向性空間プーリングを適用します。
    *   **Vertical Pooling**: 各特徴マップの幅次元で平均プーリングを行います。これにより、各「行ライン」に対応する特徴ベクトル列が、スケールごとに2セット生成されます。
    *   **Horizontal Pooling**: 各特徴マップの高さ次元で平均プーリングを行います。これにより、各「列ライン」に対応する特徴ベクトル列が、スケールごとに2セット生成されます。

3.  **1D-Decoders (1次元CNNデコーダー)**:
    *   それぞれ独立した4つの1D CNNデコーダーが、各辺（上、下、左、右）の存在確率を予測します。
    *   **Vertical Decoders (Top, Bottom)**: Vertical Poolingから得られた特徴ベクトル列 (`(batch_size, 2048, H/32)`) を入力とします。画像の各行ピクセル位置に対して「そこに上辺/下辺が存在する確率」をそれぞれ出力します。
    *   **Horizontal Decoders (Left, Right)**: Horizontal Poolingから得られた特徴ベクトル列 (`(batch_size, 2048, W/32)`) を入力とします。画像の各列ピクセル位置に対して「そこに左辺/右辺が存在する確率」をそれぞれ出力します。
    *   **デコーダー構造**: 各デコーダーは以下の構成です。
        *   `layer3` の特徴に対するデコーダ: `Linear(1024, IMAGE_SIZE)` (プーリング後の特徴ベクトルに対して適用)
        *   `layer4` の特徴に対するデコーダ: `Linear(2048, IMAGE_SIZE)` (プーリング後の特徴ベクトルに対して適用)
        *   (注: `models.py` の現在の実装では、論文に記載の1D CNNベースのデコーダーではなく、プーリング後の特徴を直接線形層に入力し、辺の位置ごとのロジットを予測後、特徴マップの空間次元で平均を取る形になっています。これは当初の1D CNNデコーダーからの変更点です。)

4.  **出力のアップサンプリング (Output Upsampling) と統合**:
    *   `layer3`由来のデコーダー出力と`layer4`由来のデコーダー出力は、それぞれがクロップ画像の各辺の位置を示すロジットベクトル（長さ `IMAGE_SIZE`）となります。
    *   これらの異なるスケールの特徴から得られた予測値を単純平均することで、最終的な各辺のロジットとします。
    *   (注: 元の論文では1D-Decoder出力のアップサンプリングがありましたが、現在の`models.py`の実装では、各スケールのプーリングされた特徴から直接 `IMAGE_SIZE`長のロジットを予測し、その後平均しています。アップサンプリング処理は明示的には行われていません。)

5.  **最終出力**:
    *   各辺（top, bottom, left, right）に対応する4つの確率分布（ロジット）。各分布は1次元ベクトルで、クロップ画像の各行/列ピクセルが辺である確率（のロジット）を表します。

### 3.2. 座標系の取り扱い

*   **JitteringされたBBox**: Ground TruthのBBoxを元に、中心位置をずらし（各軸最大10%）、寸法をランダムにスケーリング（0.9倍～1.1倍）して生成。
*   **クロップ領域**: JitteringされたBBoxをさらに `config.BUFFER_RATIO`（例: 1.1）で拡大し、この領域を元画像からクロップ。
*   **モデル入力**: クロップされた画像領域を `config.IMAGE_SIZE` x `config.IMAGE_SIZE` にリサイズしてモデルに入力。
*   **ターゲットラベルと損失計算**: 全てクロップ＆リサイズ後の画像座標系 (`config.IMAGE_SIZE` x `config.IMAGE_SIZE`) で行われます。

## 4. データ処理と拡張 (Dataset Implementation)

`src/dataset.py` 内の `CocoAdjustmentDataset` クラスがデータ処理を担当します。

1.  **データソース**: MS COCOデータセットを使用。
2.  **"Jittering" (ジッタリング)**:
    *   Ground Truthのバウンディングボックスに対して、中心位置を各軸に対して最大 `config.JITTER_CENTER_MAX_PERCENT` (例: 0.1 = 10%) ずらします。
    *   寸法（幅・高さ）を `config.JITTER_SCALE_MIN_RATIO` (例: 0.9) から `config.JITTER_SCALE_MAX_RATIO` (例: 1.1) の間のランダムな比率で再スケーリングします。
    *   これにより、ノイズの多い、あるいは不正確な初期アノテーションデータを動的に生成し、モデルのロバスト性を高めます。
    *   実装は `src/utils.py` の `jitter_bbox` 関数。

3.  **クロッピングとリサイズ (Cropping and Resizing)**:
    *   Jitteringによって生成されたバウンディングボックスを取得。
    *   このBBoxを `config.BUFFER_RATIO` (例: 1.1) で拡大し、クロップ領域を決定します。Buffer Ratioの導入目的は、GTの辺がクロップ領域の境界ギリギリになることを避け、モデルが辺を予測するための十分なコンテキストを得られるようにするためです。
    *   元画像からこのクロップ領域を切り出し、`config.IMAGE_SIZE` x `config.IMAGE_SIZE` (例: 224x224) にリサイズします。これがモデルへの入力画像となります。

4.  **ターゲットラベル生成 (Target Label Generation)**:
    *   クロップ＆リサイズ後の画像座標系において、Ground Truthの各辺（上、下、左、右）が実際に位置するピクセルインデックスを特定します。
    *   各辺に対して、`config.IMAGE_SIZE` 要素を持つ1次元のターゲットベクトルを生成します。
    *   Ground Truthの辺が存在するピクセル位置の要素を1とし、それ以外の要素を0とする、実質的なワンホットベクトル（ただし、辺が1ピクセル幅であるため厳密にはマルチホットではないが、1ピクセルのみが1となるケースが多い）として表現されます。
    *   実装は `src/utils.py` の `get_target_vector` 関数。

## 5. 学習プロセス (Training)

`src/train.py` が学習と検証のループを管理します。

*   **損失関数 (Loss Function)**:
    *   `torch.nn.BCEWithLogitsLoss` を使用。
    *   モデルが出力する各辺のロジット（アップサンプリング後）と、対応するターゲットベクトル（ワンホット）との間でバイナリクロスエントロピー損失を計算します。
    *   4つの辺（top, bottom, left, right）それぞれに対して損失を計算し、これらの合計を最終的な損失とします。

*   **オプティマイザ (Optimizer)**: AdamW (`torch.optim.AdamW`) を使用。学習率は `config.LEARNING_RATE`。
*   **学習ループ**:
    *   学習データローダーからバッチを取得。
    *   モデルに入力画像と初期BBox（ただし現行モデルは初期BBoxを直接は利用せず、クロップにのみ利用）を与え、4辺のロジットを出力。
    *   損失を計算し、バックプロパゲーションによりモデルの重みを更新。
*   **検証ループ**:
    *   各エポック終了後に検証データセットで評価。
    *   損失と各種評価指標を計算。
*   **モデル保存**: 最良の検証結果（例: IoU）を示したモデルの重みを保存。

## 6. 評価指標 (Evaluation Metrics)

`src/utils.py` に評価指標の計算関数が実装されています。

1.  **Intersection over Union (IoU)**:
    *   予測されたバウンディングボックスとGround Truthのバウンディングボックスの重なり具合を評価。
    *   モデル出力のロジットから、最も確率の高い位置を辺として選択し (`predictions_to_bboxes`)、BBoxを形成。GTも同様にターゲットベクトルからBBoxを形成 (`get_gt_bboxes_from_targets`) して計算 (`calculate_iou_batch`)。

2.  **Edge Deviance (辺のズレ)**:
    *   予測された各辺とGround Truthの各辺のピクセル単位の絶対的なズレ。
    *   平均ズレ、およびズレが1ピクセル以内/3px以内に収まる割合を計算 (`calculate_edge_deviance_batch`)。

3.  **Corner Deviance (頂点のズレ)**:
    *   予測されたバウンディングボックスの4つの頂点とGround Truthの頂点の間のピクセル単位のL1距離。
    *   平均ズレ、およびズレが1ピクセル以内/3px以内に収まる割合を計算 (`calculate_corner_deviance_batch`)。

## 7. 実装詳細とファイル構成

*   **`src/config.py`**:
    *   学習率、バッチサイズ、エポック数、画像サイズ (`IMAGE_SIZE`)、ResNetの出力チャネル数、バッファー比率 (`BUFFER_RATIO`)、ジッタリング関連のパラメータなど、主要なハイパーパラメータと設定値を集約。
*   **`src/models.py`**:
    *   `BoundingBoxAdjustmentModel` クラスの定義。ResNet-50バックボーン、Directional Spatial Pooling、1D-Decodersを含む。
*   **`src/dataset.py`**:
    *   `CocoAdjustmentDataset` クラスの定義。COCOデータセットの読み込み、画像の前処理、Jittering、クロッピング、ターゲットベクトルの生成を行う。
*   **`src/utils.py`**:
    *   `jitter_bbox`: バウンディングボックスのジッタリング処理。
    *   `get_target_vector`: クロップ後の座標系でターゲットベクトルを生成。
    *   `predictions_to_bboxes`: モデル出力（ロジット）からバウンディングボックス座標を生成。
    *   `get_gt_bboxes_from_targets`: ターゲットベクトルからGround Truthのバウンディングボックス座標を生成。
    *   `calculate_iou_batch`, `calculate_edge_deviance_batch`, `calculate_corner_deviance_batch`: 各種評価指標の計算。
*   **`src/train.py`**:
    *   学習・検証ループの実装。
    *   データローダーの準備、モデルの初期化、オプティマイザの設定。
    *   損失計算、勾配更新。
    *   検証処理と評価指標の計算、結果の表示。
    *   TensorBoardによるロギング、モデルのチェックポイント保存。

## 8. 開発過程での工夫点と課題解決

*   **核心的アイデアの理解**: AWSのブログ記事等を参考に、「辺の位置予測をピクセルごとの分類問題として扱う」という方針を確立。これにより、モデルの出力形式（各辺の確率分布のロジット）と損失関数（BCEWithLogitsLoss）が明確になった。
*   **座標系の整合性**: クロップ処理、リサイズ処理が複数回行われるため、Ground Truthの座標、JitterされたBBoxの座標、モデル入力画像の座標、損失計算時の座標系を常に意識し、整合性を保つように設計。特に、ターゲットベクトルはクロップ＆リサイズ後の座標系で生成することが重要。
*   **Buffer Ratioの導入**: JitterされたBBoxが画像の境界ギリギリになるケースを避けるため、クロップ時にマージン（Buffer Ratio）を持たせることで、モデルが辺を予測するためのコンテキストを確保。
*   **CUDA関連エラーへの対処**:
    *   `DataLoader` の `pin_memory=True` 使用時に `torch.cuda.FloatTensor` をピン留めできないエラーが発生。`dataset.py` で生成されるテンソルが確実にCPU上にあるように修正（例: `torch.tensor(..., device='cpu')`）。
    *   `multiprocessing` 使用時の "Cannot re-initialize CUDA in forked subprocess" エラーに対処するため、`train.py` で `torch.multiprocessing.set_start_method('spawn', force=True)` を設定。

## 9. 現状の評価と今後の展望 (初期学習段階)

現在のアーキテクチャで18エポックの学習が完了し、検証データセットにおいて以下の主要な結果が得られています。
実験条件の詳細は `src/config.py` を参照してください（例: `IMAGE_SIZE=224`, `LEARNING_RATE=2e-4`, `BATCH_SIZE=128`, `EPOCHS=20` など）。

*   **Average Validation Loss (Loss/validation_epoch)**: 約 0.0723
*   **Average IoU (IoU/validation)**: 約 0.877

**考察**:
*   IoUは0.877と良好な値を示しており、マルチスケール特徴の導入（`layer3` と `layer4` の利用）や18エポックの学習を通じて、初期段階（1エポック時点のIoU約0.84）よりも改善が見られます。これはモデルが大まかなバウンディングボックスの位置調整をより正確に行えるようになってきていることを示唆しています。
*   Validation Lossも微減しており、学習が順調に進んでいると考えられます。
*   ピクセルレベルの精度を示すDeviance系指標は今回省略しましたが、IoUの向上は辺の位置予測の全体的な改善を反映している可能性があります。さらなるエポック数の増加や、ハイパーパラメータの微調整、デコーダー構造の再検討（例: 論文準拠の1D CNNデコーダーへの回帰や改良）によって、より精密な調整能力の向上が期待されます。
*   `config.py` で設定されている総エポック数（`EPOCHS=20`）に向けて学習を継続することで、さらなる性能向上が見込まれます。

この記録が、今後の開発や分析の一助となれば幸いです。 
