- 学習率：1e-4
- エポック：5（途中で中止）
- バッチサイズ：64
- Buffer Ratio：1.1
- その他：Layer3とLayer4を用いる、Multi-Scale Featuresの単純な実装。詳細は以下。

## モデル実装時のチャット（Gemini-2.5-pro）

### 自分
> Multi-Scale Featureについては論文内で触れられているので、やはり活用したいですね。実装するとしたらどういう方策で行いますか。

> ### Gemini-2.5-pro
> Multi-Scale Featureの活用、良いですね！論文でも触れられている重要な要素ですので、精度向上に繋がる可能性があります。
>
> Multi-Scale Featureを実装する一般的なアプローチとしては、バックボーンネットワーク（今回はResNet-50）の複数の中間層から特徴マップを抽出し、それらを組み合わせて利用する方法があります。
>
> 具体的な実装方針として、以下のステップを提案します。これは、FPN (Feature Pyramid Network) のアイデアを少し簡略化しつつ、複数のスケールの情報を活用するものです。
>
> 1.  **ResNet-50からの特徴抽出の変更**:
>     *   現在、ResNet-50の最終出力に近い単一のスケールの特徴マップ（`layer4` の出力後、グローバルプーリング直前など）を利用していると思われます。
>     *   これを変更し、例えばResNet-50の `layer3` と `layer4` の両方の出力特徴マップを取得できるようにします。
>         *   `layer3` の出力: 比較的解像度が高く、空間的な詳細情報も保持している（例: 入力224x224に対し、14x14の解像度、1024チャネル）。
>         *   `layer4` の出力: より深い層で、意味情報が豊富だが解像度は低い（例: 入力224x224に対し、7x7の解像度、2048チャネル）。
>     *   これらの特徴マップを抽出するには、`torchvision.models.feature_extraction` モジュールを利用するのが便利です。
>
> 2.  **特徴マップの融合**:
>     *   取得した2つの異なるスケールの特徴マップを、同じ空間解像度に揃えて結合します。
>     *   例えば、`layer4` の特徴マップを `layer3` の空間解像度（例: 14x14）にアップサンプリングします（`nn.Upsample` や `nn.ConvTranspose2d` を使用）。
>     *   アップサンプリングされた `layer4` の特徴マップと、元の `layer3` の特徴マップをチャネル方向に連結 (concatenate) します。
>         *   これにより、チャネル数が `1024 (layer3) + 2048 (layer4) = 3072` で、空間解像度が `layer3` と同じ（例: 14x14）の融合された特徴マップが得られます。
>
> 3.  **Directional Spatial Pooling と 1D-Decoder への適用**:
>     *   この新しく生成された融合特徴マップ（3072チャネル）を、既存のDirectional Spatial Pooling層への入力とします。
>     *   Directional Spatial Pooling（Vertical/Horizontal Pooling）は、入力特徴マップのチャネル数をそのまま維持して1次元の特徴シーケンスを生成します。したがって、プーリング後の1D特徴ベクトルのチャネル数も3072になります。
>     *   各1D-Decoderの最初の畳み込み層 (`nn.Conv1d`) の `in_channels` パラメータを、この新しいチャネル数（3072）に合わせて変更する必要があります。
>
> **主な変更箇所**:
>
> *   `src/models.py` の `BoundingBoxAdjustmentModel` クラス:
>     *   ResNet-50の初期化部分を変更し、`feature_extraction` を用いて `layer3` と `layer4` の出力を取得するようにします。
>     *   `forward` メソッド内で、取得した特徴マップのアップサンプリングと連結処理を追加します。
>     *   1D-Decoderの最初の畳み込み層の `in_channels` パラメータを、連結後の特徴マップのチャネル数に合わせます。
>
> **考慮事項**:
>
> *   **計算コストの増加**: 複数の特徴マップを利用し、特にチャネル数が増加するため、モデルのパラメータ数と計算量が増加します。学習時間や必要なGPUメモリに影響が出る可能性があります。
> *   **ハイパーパラメータ調整**: 1D-Decoderのチャネル数や層数などのアーキテクチャパラメータ、学習率などの学習ハイパーパラメータの再調整が必要になる場合があります。
> *   **FPNの導入**: より高度な方法として、本格的なFPN構造を導入することも考えられます。これは、複数のスケールの特徴マップをより洗練された方法で融合し、各スケールで予測を行うものです。今回の提案は、その第一歩として比較的シンプルな融合方法です。
>
> この方針で実装を進めてみますか？もしよろしければ、まず `src/models.py` の変更に着手します。

> ### 自分
> FPNを導入するとしたらどういう実装になるか検討だけしてくれませんか

> ### Gemini-2.5-pro
> はい、FPN (Feature Pyramid Network) を導入する場合の実装方針について検討しましょう。
>
> FPNは、異なるスケールの特徴マップを効率的に組み合わせることで、オブジェクトのスケール変化に強い表現を獲得する手法です。Bounding Box Adjustment ModelにFPNを組み込む場合、以下のような流れが考えられます。
>
> **1. バックボーン (ResNet-50) からの複数スケール特徴抽出**
>
> *   ResNet-50の異なるステージから特徴マップを取得します。一般的にFPNでは、解像度が異なる複数の層（例: `layer2`, `layer3`, `layer4` の出力。これらを `C2, C3, C4` などと呼びます）を利用します。`layer4` の出力が最も深い（意味情報が豊富で解像度が低い）特徴マップ `C5` に対応します。
>     *   `C3`: 中間レベルの特徴。例: 入力224x224に対し、28x28の解像度、512チャネル。(ResNetのlayer2出力)
>     *   `C4`: より深い特徴。例: 入力224x224に対し、14x14の解像度、1024チャネル。(ResNetのlayer3出力)
>     *   `C5`: 最も深い特徴。例: 入力224x224に対し、7x7の解像度、2048チャネル。(ResNetのlayer4出力)
>
> **2. トップダウン経路と横方向接続 (Lateral Connection)**
>
> *   **P5の生成**: まず、最も深い特徴マップ `C5` に対して1x1畳み込みを適用し、チャネル数をFPNで統一的に扱うチャネル数（例: 256）に削減します。これを `P5` とします。
> *   **P4の生成**: `P5` を2倍にアップサンプリング（例: `nn.Upsample` を使用）します。一方、バックボーンから得た `C4` に対しても1x1畳み込みを適用してチャネル数を256に削減します。アップサンプリングされた `P5` と処理済みの `C4` を要素ごとに足し合わせます。これを `P4` とします。
> *   **P3の生成**: 同様に、`P4` を2倍にアップサンプリングし、1x1畳み込みでチャネル数を256に削減した `C3` と足し合わせます。これを `P3` とします。
> *   この処理を、利用したい最も浅い層（例: `C2` から `P2` を生成）まで繰り返します。
> *   各 `Pi` (例: `P3, P4, P5`) の後には、エイリアシング効果を低減するために3x3畳み込み層（チャネル数は256のまま）を適用することが一般的です。
>
> **3. 予測ヘッドへの接続**
>
> FPNによって生成された複数の特徴マップ (`P3, P4, P5` など) をどのようにして最終的な辺の予測に繋げるか、いくつか方針が考えられます。
>
> *   **方針A: 各スケールで独立して予測し、統合する**
>     *   各 `Pi` (例: `P3, P4, P5`) に対して、それぞれ独立したDirectional Spatial Poolingと1D-Decoderセットを適用します。
>     *   つまり、3つのスケールそれぞれでtop/bottom/left/rightの辺の予測を行います。
>     *   最終的な辺の座標は、これらの複数スケールの予測結果を統合して決定します（例: 最も信頼度の高い予測を選択、平均化、あるいは学習可能な重みで結合など）。
>     *   この場合、損失も各スケールで計算し、合計または平均します。
>
> *   **方針B: 最も適切な単一スケールを選択または特徴を集約して予測する**
>     *   論文の図では、最終的な1D-Decoderは1セットに見えます。この場合、FPNで得られた複数の特徴マップから、最もタスクに適したスケール（例: ある程度の解像度を持つ `P3` や `P4`）を選択して利用します。
>     *   あるいは、`P3, P4, P5` をさらに集約する処理（例: 全てを同じ解像度にリサイズし、チャネル方向に連結または要素ごとに加算した後、1x1畳み込みでチャネル数を調整）を行い、単一の最終的な特徴マップを生成します。この最終特徴マップを既存のDirectional Spatial Poolingと1D-Decoderに入力します。こちらの方が現在のモデル構造からの変更は少ないかもしれません。
>
> **`src/models.py` での主な変更点 (方針B-集約パターンを想定した場合)**
>
> 1.  **`BoundingBoxAdjustmentModel` の `__init__`**:
>     *   ResNet-50バックボーンの初期化で、`feature_extraction` を用いて複数のステージ (`C3, C4, C5`) の出力を取得できるようにします。
>     *   FPNの各構成要素（1x1畳み込み層、アップサンプリング層、3x3畳み込み層）を定義します。
>     *   FPNからの出力を集約するための層（例: 追加の畳込

> ### 自分
> そうですね、FPNをいきなり実装するよりも、シンプルなMulti-Scaleアプローチの実装からまず始めたいです。

> ### Gemini-2.5-pro
> わかりました。FPN（Feature Pyramid Network）を実装する前に、よりシンプルなマルチスケールアプローチを`BoundingBoxAdjustmentModel`に導入しましょう。
> 
> 現在のモデルはResNet-50の単一の層（`layer4`）からの特徴マップを使用しています。これを拡張し、複数の層（例えば`layer3`と`layer4`）からの特徴を利用するように変更します。
>
> **変更方針:**
> 
> 1.  **ResNet層の分割**: ResNet-50の各主要ステージ（`layer1`、`layer2`、`layer3`、`layer4`など）を個別にアクセスできるようにモデルの`__init__`メソッドを変更します。
> 2.  **複数スケールの特徴抽出**: `forward`メソッド内で、入力画像をResNetに通し、`layer3`の出力（`features_l3`）と`layer4`の出力（`features_l4`）を取得します。
> 3.  **スケールごとのデコーダ**:
>     *   `layer3`の特徴（チャネル数1024）を入力とする1Dデコーダ群（`decoder_l3_top`など）を定義します。
>     *   `layer4`の特徴（チャネル数2048）を入力とする1Dデコーダ群（`decoder_l4_top`など）を定義します。これは既存のデコーダと同様の構成になります。
> 4.  **スケールごとの予測**:
>     *   `features_l3`に対してDirectional Spatial Poolingを適用し、`decoder_l3`群で辺の予測（`preds_top_l3`など）を行います。
>     *   `features_l4`に対して同様にDirectional Spatial Poolingを適用し、`decoder_l4`群で辺の予測（`preds_top_l4`など）を行います。
>     *   各スケールでの予測時、プーリング後の特徴マップの空間次元（例：`W_feat`）に沿って得られる複数の予測値を平均し、最終的に各辺に対して`(バッチサイズ, IMAGE_SIZE)`の形状の予測を得ます。これは現在のモデルの暗黙的な動作と一致します。
> 5.  **予測の統合**: `layer3`からの予測値と`layer4`からの予測値を単純平均し、最終的なモデル出力とします（例：`final_preds_top = (preds_top_l3 + preds_top_l4) / 2.0`）。
> 6.  **出力形式の維持**: モデルの最終的な出力形式は、既存の `(preds_top, preds_bottom, preds_left, preds_right)`（各テンソルの形状は `(バッチサイズ, config.IMAGE_SIZE)`）を維持します。これにより、学習ループや評価関数など、他のコンポーネントの変更は不要です。
> 
> このアプローチにより、モデルは異なるスケールの特徴情報を活用しつつ、比較的シンプルな構造を保ちます。
> 
> それでは、`src/models.py`の`BoundingBoxAdjustmentModel`クラスを上記方針で修正します。
> 
> `src/models.py`を以下のように変更します。
> 
> `models.py` が大幅に更新され、ResNet-50の`layer3`と`layer4`からの特徴を利用するマルチスケールアーキテクチャになりました。また、デコーダの構造も以前の`Conv1D`ベースから`nn.Linear`ベースに変更され、出力形式も辞書型からタプル型に変わりました。
> 
> この変更に伴い、学習スクリプト `src/train.py` が新しいモデルの出力形式に対応できるように修正が必要です。具体的には、モデルからの出力（各辺の予測確率）をタプルとして受け取るように変更します。
> 
> `src/train.py` の `train_one_epoch` 関数と `validate_one_epoch` 関数内で、モデルの出力を受け取っている部分を以下のように修正します。
> 
> 変更前（例）:
> ```python
> outputs = model(images_gpu)
> preds_top = outputs['top']
> preds_bottom = outputs['bottom']
> preds_left = outputs['left']
> preds_right = outputs['right']
> ```
> 
> 変更後:
> ```python
> preds_top, preds_bottom, preds_left, preds_right = model(images_gpu)
> ```
> モデルのインスタンス化については、新しいモデルは `config` オブジェクトを引数に取るため、`train.py` で `model = BoundingBoxAdjustmentModel(config=cfg)` のように初期化されていることを確認します（おそらく現在のコードでも同様の方法で初期化されていると考えられます）。
> 
> `src/train.py` の修正箇所は主にモデル出力のアンパッキングです。以下に修正を提案します。
> 
> `src/train.py` の修正が適用され、モデルの出力形式の変更に対応できました。
> 
> 具体的には、`train_one_epoch` 関数と `validate_one_epoch` 関数の両方で、モデルからの出力をタプル `preds_top, preds_bottom, preds_left, preds_right` として正しく受け取り、損失計算に使用するように変更されました。
> 
> また、`validate_one_epoch` 関数内で評価指標計算のために `utils.predictions_to_bboxes` 関数を呼び出す際、以下のように修正後のタプル形式の予測結果から辞書を再構築して渡すように変更されています。
> ```python
> pred_bboxes_batch = utils.predictions_to_bboxes(
>     {
>         "top": preds_top,
>         "bottom": preds_bottom,
>         "left": preds_left,
>         "right": preds_right
>     }, image_width, image_height, device
> )
> ```
> これは、`utils.predictions_to_bboxes` 関数が依然として辞書形式の入力を期待しているという仮定に基づいた適切な対応です。
> 
> 以上の変更により、新しいマルチスケールモデル `BoundingBoxAdjustmentModel` を用いて学習と検証が実行可能になったはずです。
> 
> これで、ご要望の「シンプルなMulti-Scaleアプローチの実装」が完了しました。この状態で学習を試してみて、結果を確認いただけますでしょうか。
> もし他に何かご希望や修正点があれば、お申し付けください。
